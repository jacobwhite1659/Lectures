{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 22 \n",
    "- Vector Projection\n",
    "- Span\n",
    "- Bases\n",
    "- Gram-Schmidt Process\n",
    "- Rotation Matrix\n",
    "- Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "def plotvec(*argv):\n",
    "    colors=['k','b','r','g','c','m']\n",
    "    xmin=0\n",
    "    xmax=-1000000\n",
    "    ymin=0\n",
    "    ymax=-1000000\n",
    "    origin=[0,0]\n",
    "    plt.figure()\n",
    "    for e in enumerate(argv):\n",
    "        i=e[0]\n",
    "        arg=e[1]\n",
    "        plt.quiver(*origin,*arg,angles='xy',scale_units='xy',scale=1,\n",
    "                   color=colors[i%len(colors)])\n",
    "        xmin=min(xmin,arg[0])\n",
    "        xmax=max(xmax,arg[0])\n",
    "        ymin=min(ymin,arg[1])\n",
    "        ymax=max(ymax,arg[1])\n",
    "    plt.xlim(min(-1, xmin-1), max(1,xmax+1))\n",
    "    plt.ylim(min(-1,ymin-1),max(1,ymax+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Projection, Spanning Sets and Bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the angle relation for 2-vectors introduced last week:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}^T\\mathbf{y} = \\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert\\cos\\theta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Vector Correlation</b>\n",
    "\n",
    "Then the **vector correlation** between $\\mathbf{x}$  and $\\mathbf{y}$ is \n",
    "\n",
    "\\begin{align*}\n",
    "r = \\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{x}\\Vert\\Vert\\mathbf{y}\\Vert} = \\cos(\\theta)\n",
    "\\end{align*}\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why would we call this vector correlation? First, let's write out the operations of vector correlation:\n",
    "\n",
    "\\begin{align*}\n",
    "r &= \\frac{  \\sum_{i} x_i y_i }\n",
    "{\\sqrt{ \\sum_{i} x_{i}^{2}}\\sqrt{ \\sum_{j}y_{j}^{2}}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall the form of Pearson's correlation:\n",
    "\\begin{align*}\n",
    "\\rho_{xy} &= \\frac{\\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_X \\sigma_Y} \\\\\n",
    "&= \\frac{ \\frac{1}{N-1} \\sum_{i} (x_i- \\mu_x)(y_i -\\mu_y)}\n",
    "{\\sqrt{\\frac{1}{N-1} \\sum_{i} (x_i- \\mu_x)^2}\\sqrt{\\frac{1}{N-1} \\sum_{j}(y_j -\\mu_y)^2}}\n",
    "\\end{align*}\n",
    "\n",
    "If $\\mu_x=\\mu_y=0$, this simplifies to \n",
    "\\begin{align*}\n",
    "\\rho_{xy} &= \\frac{\\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_X \\sigma_Y} \\\\\n",
    "&= \\frac{  \\sum_{i} x_i y_i }\n",
    "{\\sqrt{ \\sum_{i} x_{i}^{2}}\\sqrt{ \\sum_{j}y_{j}^{2}}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "So, if the vectors have been normalized to have zero-mean, **vector correlation** and **Pearson's correlation** have the same exact form! \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, for 2-vectors separated by angle $\\theta$,\n",
    "\n",
    "\\begin{align*}\n",
    "r= \\cos{\\theta},\n",
    "\\end{align*}\n",
    "\n",
    "which implies that\n",
    "\n",
    "\\begin{align*}\n",
    "-1 \\le r \\le 1\n",
    "\\end{align*}\n",
    "\n",
    "just like Pearson's correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Although we can only calculate the angle for 2-vectors, **correlation is a measure of similarity** between two vectors of **any dimensionality**\n",
    "\n",
    "For instance, if $\\mathbf{y}=c \\mathbf{x}$ for some $c \\neq 0 $ a scalar value, the correlation between $\\mathbf{x}$ and $\\mathbf{y}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "r &= \\frac{  \\sum_{i} x_i (cx_i) }\n",
    "{\\sqrt{ \\sum_{i} x_{i}^{2}}\\sqrt{ \\sum_{j}(cx_{j})^{2}}} \\\\\n",
    " &= \\frac{  c\\sum_{i} x_{i}^{2} }\n",
    "{\\sqrt{ \\sum_{i} x_{i}^{2}}\\sqrt{ c^2 \\sum_{j}x_{j}^{2}}} \\\\\n",
    "&= 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then important that we work with normalized vectors, or **unit vectors**.\n",
    "\n",
    "### Normalizing a vector\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "A vector $\\mathbf{x}$ is a **unit vector** if $\\|\\mathbf{x} \\|=1$.\n",
    "</div>\n",
    "\n",
    "If $\\|\\mathbf{x} \\| \\ne 1$, we can create a unit vector in the same direction as $\\mathbf{x}$ as\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{\\mathbf{x}} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|}\n",
    "\\end{align*}\n",
    "\n",
    "because\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\| \\tilde{\\mathbf{x}} \\right\\| &= \\left\\| \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|} \\right\\| \\\\\n",
    "&=  \\frac{\\left\\|\\mathbf{x} \\right\\| }{\\|\\mathbf{x}\\|}\\\\ \n",
    "&=1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Projection\n",
    "\n",
    "Then if we use $\\tilde{\\mathbf{x}}$ instead, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y}^T\\tilde{\\mathbf{x}} & = \\left\\Vert\\frac{\\mathbf{x}}{\\| \\mathbf{x} \\|}\\right\\Vert \\Vert\\mathbf{y}\\Vert\\cos\\theta \\\\\n",
    "& =  \\Vert\\mathbf{y}\\Vert\\cos \\theta \n",
    "\\end{align*}\n",
    "\n",
    "We call this *displacement* amount in the direction of $\\tilde{\\mathbf{x}}$ for the vector **projection** of $\\mathbf{y}$ onto $\\tilde{\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Vector Projection</b>\n",
    "\n",
    "The **vector projection** of $\\mathbf{y}$ onto $\\mathbf{x}$ is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{proj}_{\\mathbf{x}}\\mathbf{y} &= (\\mathbf{y}^T\\tilde{\\mathbf{x}})\\tilde{\\mathbf{x}}\n",
    "\\end{align*}\n",
    "    \n",
    "where $\\tilde{\\mathbf{x}} = \\frac{\\mathbf{x}}{\\Vert\\mathbf{x}\\Vert}$. We can then write,\n",
    "    \n",
    "\\begin{align*}\n",
    "\\text{proj}_{\\mathbf{x}}\\mathbf{y} &= \\frac{\\mathbf{y}^T\\mathbf{x}}{\\Vert\\mathbf{x}\\Vert} \\frac{\\mathbf{x}}{\\Vert\\mathbf{x}\\Vert} = \\frac{\\mathbf{y}^T\\mathbf{x}}{\\Vert\\mathbf{x}\\Vert^2} \\mathbf{x}\n",
    "\\end{align*}\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what that means pictorially using the *virtual whiteboard*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{x}=[5,2]^T$ and $\\mathbf{y}=[3,4]^T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find and visualize the projection of $\\mathbf{y}$ onto $\\tilde{\\mathbf{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numpy, we can find the norm of a vector with ```np.linalg.norm()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's import ```numpy.linalg``` as ```la```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plt.legend(['$x$','$y$',\"$x'$\"],fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the projection of $\\mathbf{y}$ onto $\\tilde{\\mathbf{x}}$ is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show this as a projection onto the vector $\\mathbf{x}$, we just need to scale $\\tilde{\\mathbf{x}}$ by this projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.legend(['$x$','$y$',\"$proj_{x}y$\"],fontsize=15);\n",
    "plt.xlim(-2,8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "So we can represent *any* vector in the direction of a unit vector by performing vector projection!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to represent a vector $\\mathbf{y}$ using a new set of axes.\n",
    "\n",
    "Suppose our new axes are represented by **unit vectors** called $\\mathbf{x}'$ and $\\mathbf{y}'$ (here the primes don't have anything to do with derivatives).\n",
    "\n",
    "Let $\\mathbf{x}'$ be at an angle that is $60^\\circ$ from the usual $x$-axis.\n",
    "\n",
    "Let's find $\\mathbf{x}'$ and $\\mathbf{y}'$ using the *virtual whiteboard*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.legend([\"$x'$\",'new $x$','new $y$'], fontsize=15);\n",
    "plt.xlim([-1.5,2])\n",
    "plt.ylim([-0.2,1.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that $\\mathbf{x}'$ and $\\mathbf{y}'$ are orthogonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Orthonormal Vector</b>\n",
    "    \n",
    "Since $\\mathbf{x}'$ and $\\mathbf{y}'$ are orthogonal and have been normalized, we say that $\\{\\mathbf{x}', \\mathbf{y}'\\}$ is a set of **orthonomal** vectors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's represent a vector using these new axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.array([2,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar value in the direction of the new x-axis\n",
    "\n",
    "\n",
    "# Scalar value in the direction of the new y-axis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize these projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting z in the new x-axis\n",
    "\n",
    "\n",
    "# Projecting z in the new y-axis\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(['new $x$','new $y$','$z$','Projection of $z$ on new $x$',\n",
    "            'Projection of $z$ on new $y$'], loc='lower left');\n",
    "plt.xlim([-3,3]); plt.ylim([-2,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can completely reconstruct $\\mathbf{z}$ from\n",
    "$\\mathbf{z}'$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.legend(['$z$','reconstructed $z$']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we made a new vector from these projections? What would it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.legend(['new $x$','new $y$','$z$','Projection of $z$ on new $x$',\n",
    "            'Projection of $z$ on new $y$','new $z$'],loc='lower left')\n",
    "plt.xlim([-3,3]); plt.ylim([-2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the angle between $\\mathbf{z}$ and $\\mathbf{z}'$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using the representation of $\\mathbf{z}$ on these new axes is the same as **rotating** $\\mathbf{z}$ by the amount the axes are rotated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the representation has the same length as the original $\\mathbf{z}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually do both inner products in one step.\n",
    "\n",
    "Let's stack the vectors $\\mathbf{x}'$ and $\\mathbf{y}'$ side by side into an array.\n",
    "\n",
    "First we need to convert to 2-D vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now carrying out the inner product of the axes array (must be on left-hand side) with the $\\mathbf{z}$ vector will do both vector inner products and stack the results into a new vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that any vector in $\\mathbb{R}^2$ can be represented using these new axes. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Span</b>\n",
    "    \n",
    "We say that $\\mathcal{S}=\\{\\mathbf{x}', \\mathbf{y}'\\}$ is a **spanning set** for $\\mathbb{R}^2$ (or say that $\\mathcal{S}$ **spans**  $\\mathbb{R}^2$).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimum of 2 spanning vectors are required to represent everything in $\\mathbb{R}^2$. (We say that the **dimension** of $\\mathbb{R}^2$ is 2.)\n",
    "\n",
    "Since the cardinality of $\\mathcal{S}$ is $\\left| \\mathcal{S} \\right|=2$, $\\mathcal{S}$ is **minimal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that $\\mathcal{S}$ is a **minimal spanning set** or a **basis** for $\\mathbb{R}^2$.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Basis Set</b>\n",
    "    \n",
    "If $\\mathcal{S}$ is a **minimal spanning set**, if the vectors $\\mathcal{S}$ are orthonormal, we say that $\\mathcal{S}$ is an orthonormal basis for $\\mathbf{R}^D$, where $D=|\\mathcal{S}|$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we care?**\n",
    "\n",
    "* Often we have high-dimensional data, and we want to apply the two-dimensional techniques that we know (like 2-D regression) to that data.\n",
    "\n",
    "* Or we may want to perform data compression.\n",
    "\n",
    "* We can do that if we can reduce the dimensionality, which corresponds to not using some dimensions. \n",
    "\n",
    "* But it may not be a good idea to completely throw away one element of each vector. \n",
    "\n",
    "* Instead, we may want to represent the data using a different basis and throw away one dimension in that representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps**\n",
    "\n",
    "1. We know one orthonormal basis for $\\mathbb{R}^3$. How can we find another one?\n",
    "2. Suppose we have a set of vectors. How can we find an orthonormal basis for them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try to get some insight by finding a new basis for $\\mathbf{R}^3$. \n",
    "\n",
    "Start with some random vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will treat these as 3 horizontally stacked vectors. Let's start by creating a new \"x\" axis from the first vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we tried to do that with the second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 and y1 are not orthogonal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use y1 to make a new vector that is orthogonal to x1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's project y1 onto x1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the error vector should be orthogonal to that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize e1 to get an orthogonal vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let $x_2=x_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the same the third vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is not orthogonal to either of previous 2. Let's find the part that is orthogonal to both $x_1$ and $y_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and normalize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have a new orthonormal basis for $\\mathbb{R}^3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we take any vector in $\\mathbb{R}^3$, we can represent it using these basis vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=np.array([2,2,-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure we conducted above will work for **any** set of vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram-Schmidt Process\n",
    "\n",
    "Given indexed $n$-vectors $a_0, a_1, \\ldots, a_{K-1}$. \n",
    "1. Let $i=0$. Let $\\mathcal{Q}=()$ be the ordered collection of basis vectors (initalized to empty).\n",
    "2. For $j=0, \\ldots, |Q|-1$: calculate the correlations between vector $\\mathbf{a}_i$ and all the basis vectors: $r_{ij}=\\mathbf{a}_{i}^{T}q_j$ \n",
    "3. Calculate the error vector $\\mathbf{e}_i$, which is the part of $\\mathbf{a}_i$ that is orthogonal to all the basis vectors up to this point: $\\mathbf{e_i} = \\mathbf{a}_i - (r_{i0} \\mathbf{q}_0 + r_{i1} \\mathbf{q}_1 +\\cdots)$\n",
    "4. If $\\|\\mathbf{e}_i\\|=0$, then $\\mathbf{a}_i$ can be completely represented in terms of the basis vectors in $\\mathbf{Q}$. Increment $i$ ($i=i+1$) and go to step 2.\n",
    "5. Else normalize the error vector to create a new basis vector:\n",
    "$$\n",
    "\\mathbf{q}_{|\\mathcal{Q}|} = \\mathbf{e}/ \\left| \\mathbf{e}\\right\\|\n",
    "$$\n",
    "and go to step 2.\n",
    "\n",
    "*Note that the Boyd's book has a condensed version of Gram-Schmidt in Section 5.4, but the algorithm is not quite correct and does not give any insight into the steps*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example:</font> Consider the following 3 5-dimensional vectors. Let's find a basis set to fully characterize all vectors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=np.array([[-0.28754584,  0.39586495,  0.84738342, -0.39586495, -1.40722101],\n",
    "       [ 0.32374882,  0.31602749,  0.12318154, -0.31602749, -0.57011189],\n",
    "       [ 1.03297076,  0.05444138, -0.95597902, -0.05444138,  0.87898728]]).T\n",
    "\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First basis  vector -- must be a unit vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the projection scaling to the first basis vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the error of projection to first basis vector\n",
    "# This will be orthogonal to the first basis vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it is a unit vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the next vector, as the projection of 1st and 2nd basis vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the orthogonal error vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the error vector is normalized\n",
    "# Check if it's different than zero\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Very small value, attributable to floating point error. \n",
    "\n",
    "**Conclusion:** \n",
    "\n",
    "* We only need 2 basis vectors to represent all three input vectors!\n",
    "\n",
    "* Although we have 3 vectors from $\\mathbb{R}^5$, the cardinality of the basis is only 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to understand what determines the **cardinality** of the **basis** for a set of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Dependence\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Linearly Dependent</b>\n",
    "\n",
    "We say that a collection of vectors $\\mathbf{a}_0, \\mathbf{a}_1, \\ldots, \\mathbf{a}_{k-1}$ is **linearly dependent** if \n",
    "there exist nonzero constants $\\beta_0, \\beta_1, \\ldots, \\beta_{k-1}$ such that\n",
    "\n",
    "$$ \\beta_0 \\mathbf{a}_0 + \\beta_1 \\mathbf{a}_1 + \\cdots \\beta_{k-1} \\mathbf{a}_{k-1} =\\mathbf{0}$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that if we solve for one of these vectors, it means that we can express it as a linear combination of the other vectors. \n",
    "\n",
    "For convenience, consider solving for $\\mathbf{a}_0$:\n",
    "\n",
    "$$ \\mathbf{a}_0 = - \\biggl(\\frac{\\beta_1}{\\beta_0} \\mathbf{a}_1 + \\cdots + \\frac{\\beta_{k-1}}{\\beta_0} \\mathbf{a}_{k-1} \\biggr) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Linearly Independent</b>\n",
    "\n",
    "We say that a collection of vectors $\\mathbf{a}_0, \\mathbf{a}_1, \\ldots, \\mathbf{a}_{k-1}$ is **linearly independent** if they are not linearly dependent. In other words, the equation \n",
    "\n",
    "$$ \\beta_0 \\mathbf{a}_0 + \\beta_1 \\mathbf{a}_1 + \\cdots \\beta_{k-1} \\mathbf{a}_{k-1} =0 $$\n",
    "\n",
    "only holds if $\\beta_0 =\\beta_1= \\cdots= \\beta_{k-1}=\\mathbf{0}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we see from our example above, it can be difficult to look at a set of vectors and determine if they are linearly dependent.\n",
    "\n",
    "However, some cases are simple:\n",
    "* One non-zero vector is said to be linearly independent\n",
    "* Two non-zero vectors are linearly independent unless they are scaled versions of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Dimensionality</b>\n",
    "\n",
    "The *dimensionality* of a set of vectors is the **cardinality** of the **largest linearly independent set** of those vectors\n",
    "\n",
    "* This means that for a set of vectors of cardinality $k$, the dimension is $\\le k$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<font color=blue>Example 1:</font> Consider the set of all real $n$-vectors, $\\mathbb{R}^n$. What is its dimension?**\n",
    "\n",
    "Well, $\\mathbb{R}^n$ contains the standard unit vectors: $[1,0,0,\\ldots,0],~[0,1,0,\\ldots,0,0], \\ldots, [0,0,0,\\ldots,0,1]$\n",
    "\n",
    "Clearly, these vectors are linearly independent, so the dimension of $\\mathbb{R}^n$ is $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is the orthonormal basis found by Gram-Schmidt a set of linearly independent vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert-success\">\n",
    "\n",
    "**Orthogonal vectors are linearly independent!**\n",
    "    \n",
    "* Because they cannot be written as a linear combination of the other.\n",
    "</div>\n",
    "\n",
    "**Proof:** Let $\\mathbf{u}$ and $\\mathbf{v}$ be **nonzero** vectors that are orthogonal, that is, $\\mathbf{u}^T\\mathbf{v}=0$.\n",
    "\n",
    "Let's suppose they are linearly dependent. Then there exist nonzero constants $\\alpha$ and $\\beta$ such that\n",
    "\n",
    "$$ \\alpha \\mathbf{u} + \\beta\\mathbf{v} = 0 $$\n",
    "\n",
    "If we take the inner product of both sides with one of the vectors. Let's use $\\mathbf{u}$:\n",
    "\n",
    "$$ \\alpha \\mathbf{u}^T \\mathbf{u} + \\beta \\mathbf{u}^T \\mathbf{v} = 0 \\mathbf{u}^T$$\n",
    "\n",
    "By the definition of orthogonal vectors, $\\mathbf{u}^T\\mathbf{v}=0$.  Furthermore $\\mathbf{u}^T\\mathbf{u} = \\|\\mathbf{u}\\|^2$:\n",
    "\n",
    "$$ \\alpha \\|\\mathbf{u}\\|^2 = 0$$\n",
    "\n",
    "Since $\\alpha \\ne 0$, it must be that $\\| \\mathbf{u} \\|^2 = 0$, which can only occur if $\\mathbf{u}=\\mathbf{0}$.\n",
    "\n",
    "That contradicts our original assumptions, so it must be that **orthogonal vectors are always linearly independent!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Since the basis found by Gram-Schmidt is a set of linearly independent vectors and since any other vector in the original set of vectors can be written as a linear combination of the basis vectors:\n",
    "\n",
    "<div class=\"alert-success\">\n",
    "    \n",
    "**the dimensionality of a set of vectors is the cardinality of the basis found  by Gram-Schmidt**    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert-info\">\n",
    "\n",
    "**Gram-Schmidt gives us a way to determine the dimensionality of a set of vectors and to determine an orthonormal basis for those vectors**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**BUT**\n",
    "\n",
    "<div class=\"alert-danger\">\n",
    "    \n",
    "**Linearly independent vectors are not always orthogonal**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example 2:</font> Consider the vectors $[1,1,0]^T$ and $[0,1,1]^T$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "u=np.array([1,1,0])\n",
    "v=np.array([0,1,1])\n",
    "\n",
    "u@v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If these vectors were linearly dependent then $\\alpha \\mathbf{u} + \\beta \\mathbf{v} =\\mathbf{0}$, which implies:\n",
    "\n",
    "$$\\begin{cases} \\alpha (1) + \\beta(0) &=0\\\\ \\alpha (1) + \\beta(1) &=0\\\\ \\alpha (0) + \\beta(1) &=0 \\end{cases}$$\n",
    "\n",
    "By the first and third equations, $\\alpha=0$ and $\\beta=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If a set of vectors $\\mathcal{V}$ is linearly independent, then we can find a set of orthonormal vectors of the same cardinality by applying the Gram-Schmidt process to $\\mathcal{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence-Dimension Inequality\n",
    "\n",
    "A linearly independent collection of $n$-vectors can have at most $n$ elements.\n",
    "\n",
    "The maximum dimension of a collection of $n$-vectors is $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose you have a collection of vectors $\\mathcal{V}$ from $\\mathbb{R}^n$, where $| \\mathcal{V}| \\ge n$.  If a basis for $\\mathcal{V}$ has cardinality $n$, then that basis is a basis for $\\mathbb{R}^n$.\n",
    "\n",
    "**Proof** Since $\\mathcal{V} \\subset \\mathbb{R}^n$, then the basis for $\\mathcal{V}$ is a linearly independent set of vectors in $\\mathbb{R}^n$ of maximum cardinality: $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the famously studied [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "# print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just look at the first 2 classes and the first 2 features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each 2-vector of data in the iris data set is in row. Let's put it in columns to match our usual convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first two features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose one of these features as a variable to distinguish between the two classes, it will not be enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "#\n",
    "\n",
    "plt.title('Sepal Length', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "#\n",
    "\n",
    "plt.title('Sepal width',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can see from the figure that these classes can be separated by a line. Just not a line that is parallel to the coordinate axes.\n",
    "\n",
    "* If we represent these classes using a **rotated set of axes**, then we can perform data reduction to one-dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous lecture, we saw that the vectors for axes rotated **counterclockwise** by $\\theta$ degrees are:\n",
    "\n",
    "$$\\mathbf{x}'=[\\cos \\theta, \\sin\\theta]^T$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\mathbf{y}'=[\\cos (\\theta + 90^\\circ), \\sin(\\theta+90^\\circ)]^T$$\n",
    "\n",
    "Applying standard trignometry identities, we have:\n",
    "\n",
    "$$\\mathbf{y}'=[-\\sin (\\theta ), \\cos(\\theta)]^T$$\n",
    "\n",
    "Thus, the vertically stacked vectors form the following rotation array:\n",
    "\n",
    "$$R = \\left[\\mathbf{x} | \\mathbf{y}\\right] = \\left[\\begin{array}{cc} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makerot(theta):\n",
    "    '''This function creates a 2x2 rotation \n",
    "    matrix for a given angle (theta) in degrees'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to perform the dot product of these axes vectors with a single vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotating by $\\theta$ and then rotating by $-\\theta$ should return the original value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the combined operation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since dot product is associative, we could instead do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can find the combined matrix operation for rotating and derotating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first encounter with an **identity matrix**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Identity matrix</b>\n",
    "\n",
    "An identity matrix $\\mathbf{I}_k$ is a $k \\times k$ matrix that has ones on the diagonal and zeros everywhere else.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not quite ready to fully explore identity matrices yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the original vector and its representation on the rotated axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we hstack vectors on the right-hand side, too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is horizontally stacked vectors, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.legend(['Original $x$','Rotated $x$','Original $y$','Rotated $y$'],\n",
    "          loc='lower left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rotate all the vectors in the iris data set by taking the inner product of the basis vectors (in columns of an array) with data in columns of an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, plot the rotated data, and find a rotation that makes the two clusters separable using only the x-axis value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find a good value that allows data to be \n",
    "# separated using only x-axis info\n",
    "\n",
    "rotated=axes@X\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[0,:], X[1,:], c=y, cmap=plt.cm.Set1,edgecolor='k')\n",
    "plt.xlabel('Sepal length',size=15); plt.ylabel('Sepal width', size=15)\n",
    "plt.title('Original',size=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(rotated[0, :], rotated[1,:], c=y, cmap=plt.cm.Set1, edgecolor='k')\n",
    "plt.xlabel('New $x$',size=15); plt.ylabel('New $y$', size=15)\n",
    "plt.title('Rotated by $-40^{\\circ}$',size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to do data reduction to only one feature (corresponding to the y-axis). To do that, we just conduct the inner product with only the first basis vector from the rotation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the output is a vector of one feature per input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "plt.title('Best (New) Feature from Rotated Data', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our perspective of arrays as horizontally stacked vectors, we can do inner products with any numbers of vectors and get out the corresponding inner products.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.array([[3,7], # two 3-vectors\n",
    "            [1,1],\n",
    "            [0,-2]])\n",
    "\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.array([[2,2,2,2], # four 3-vectors\n",
    "            [-1,2,-1,2],\n",
    "            [5,3,2,4]])\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $i$th row represents all the inner product associated with the $i$th vector in G:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $j$th column represents all the inner products associated with the $j$th vector in H:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the $(i,j)$-th entry in the output matrix is the dot product from the $i$th left vector and the $j$th right vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
