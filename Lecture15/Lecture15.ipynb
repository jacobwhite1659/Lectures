{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15\n",
    "\n",
    "- Hypothesis tests\n",
    "- Trade-offs in hypothesis testing\n",
    "- Goodness-of-fit measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We visualized different **moments** of PDFs.\n",
    "\n",
    "* We used **KDE** (kernel density estimation) with a dataset (non-parametric inference of the PDF).\n",
    "\n",
    "* We learnt about **Statistical Inference** (parametric inference of the PDF) - how do estimate the moments of a PDF given data?\n",
    "\n",
    "* We learnt about the **Z-test** - how do we show that two datasets come from the same distribution (or not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Z-Test</b>\n",
    "    \n",
    "A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution. Z-tests test the mean of a distribution.\n",
    "\n",
    "* Let $\\hat{\\mu}_X$ and $\\hat{\\mu}_Y$ be the sample means of random samples of sizes $M$ and $N$ from two RVs $X$ and $Y$, respectively, with common variance $\\sigma^2$. We can build the statistic:\n",
    "\n",
    "\\begin{align*}\n",
    "T = \\hat{\\mu}_X - \\hat{\\mu}_Y\n",
    "\\end{align*}\n",
    "\n",
    "where $E[T] = 0$, $\\operatorname{Var}[X] = \\sigma^2\\left(\\frac{1}{N} + \\frac{1}{M}\\right)$ and \n",
    "\n",
    "\\begin{align*}\n",
    "T \\sim G\\left(0, \\sigma^2\\left(\\frac{1}{N} + \\frac{1}{M}\\right)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* We will continue with the **Z-test** (known variance)\n",
    "* We will see the **T-test** (unknown variance)\n",
    "* We will visualize **trade-offs in hypothesis testing**\n",
    "* We will look at some **goodness-of-fit measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Z-Test</font>: Binary Hypothesis Tests involving Sample Mean *with Known and Equal Variances*\n",
    "\n",
    "Suppose we have two populations characterized by RVs $X$ and $Y$, and the following samples $\\{x_i\\}_{i=1}^M$ and $\\{y_j\\}_{j=1}^N$, where $x_i$ and $y_j$ are observed values of RVs $X$ and $Y$, which are assumed to have common variance $\\sigma^2$.\n",
    "\n",
    "* Let the averages of the data samples be\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{x} = \\frac{1}{M}\\sum_{i=1}^{M} x_i \\text{, and } \\bar{y} = \\frac{1}{N}\\sum_{j=1}^{N} y_j\n",
    "\\end{align*}\n",
    "\n",
    "and denote the true means of the distributions $\\mu_X$ and $\\mu_Y$, respectively.\n",
    "\n",
    "* Note that if the number of samples from each population is relatively large ($\\geq 10$), then even if the original population does not have a Gaussian distribution, the averages will still be approximately Gaussian - Central Limit Theorem (CLT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If $\\bar{x} \\neq \\bar{y}$, how can we conduct a binary hypothesis test on whether the two populations have different means?**\n",
    "\n",
    "* What is the null hypothesis?\n",
    "\n",
    "    * $H_0$: **the means are the same, $\\mu_X = \\mu_Y$**\n",
    "    * $H_1$: **the means are not the same, $\\mu_X \\neq \\mu_Y$**\n",
    "    \n",
    "* We will conduct this test only using the sample observations $\\{x_i\\}_{i=1}^M$ and $\\{y_j\\}_{j=1}^N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the null hypothesis, we compute the difference in the sample averages and determine the probability that a difference that large would be observed under the null hypothesis.\n",
    "\n",
    "Thus, our test statistic is the difference in averages\n",
    "\n",
    "\\begin{align*}\n",
    "t = \\bar{x} - \\bar{y}\n",
    "\\end{align*}\n",
    "\n",
    "* Let $\\hat{\\mu}_X$ and $\\hat{\\mu}_Y$ be the sample means of random samples of sizes $M$ and $N$ from $X$ and $Y$ RVs, respectively. We can view $t$ as an instantiation of\n",
    "\n",
    "\\begin{align*}\n",
    "T = \\hat{\\mu}_X - \\hat{\\mu}_Y\n",
    "\\end{align*}\n",
    "\n",
    "If $\\mu_X = \\mu_Y = \\mu$, then $E[\\hat{\\mu}_X] = E[\\hat{\\mu}_Y] = \\mu$. Then, by linearity\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_T = E[T] = E[\\hat{\\mu}_X - \\hat{\\mu}_Y] = E[\\hat{\\mu}_X] - E[\\hat{\\mu}_Y] = \\mu - \\mu = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can compute the variance of $T$ under the null hypothesis as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma_T^2 &= Var[T] \\\\\n",
    "&= Var[\\hat{\\mu}_X - \\hat{\\mu}_Y] \\\\\n",
    "&= Var[\\hat{\\mu}_X + (-\\hat{\\mu}_Y)] \\\\\n",
    "&= Var[\\hat{\\mu}_X] + Var[-\\hat{\\mu}_Y] \\\\\n",
    "&= Var[\\hat{\\mu}_X] + (-1)^2 Var[\\hat{\\mu}_Y] \\\\\n",
    "&= \\frac{\\sigma^2}{M} + \\frac{\\sigma^2}{N} \\\\\n",
    "& = \\left( \\frac{1}{M} + \\frac{1}{N} \\right) \\sigma^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the probability of observing a difference in means as large as $t = \\bar{x} - \\bar{y}$. For convenience of discussion, assume $\\bar{x} > \\bar{y}$:\n",
    "\n",
    "Let $t$ be the observed difference $\\bar{x}-\\bar{y} > 0$.\n",
    "\n",
    "Hypothesis test:\n",
    "\n",
    "* What is $P(\\text{see result as extreme under }H_0)$\n",
    "\n",
    "    * One-sided Hypothesis test: \n",
    "    \n",
    "\\begin{align*}\n",
    "P(T \\geq t | H_0) = Q\\left(\\frac{t-\\mu_T}{\\sigma_T}\\right) = Q\\left(\\frac{t}{\\sigma \\sqrt{\\frac{1}{M}+\\frac{1}{N}}}\\right)\n",
    "\\end{align*}\n",
    "    \n",
    "    * Two-sided Hypothesis test:\n",
    "    \n",
    "\\begin{align*}\n",
    "P(|T| \\geq t | H_0) = 2 Q\\left(\\frac{t}{\\sigma \\sqrt{\\frac{1}{M}+\\frac{1}{N}}}\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Z-Test</b>\n",
    "    \n",
    "A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution. Z-tests test the mean of a distribution.\n",
    "\n",
    "* Let $\\hat{\\mu}_X$ and $\\hat{\\mu}_Y$ be the sample means of random samples of sizes $M$ and $N$ from two RVs $X$ and $Y$, respectively, with common variance $\\sigma^2$. We can build the statistic:\n",
    "\n",
    "\\begin{align*}\n",
    "T = \\hat{\\mu}_X - \\hat{\\mu}_Y\n",
    "\\end{align*}\n",
    "\n",
    "where $E[T] = 0$, $\\operatorname{Var}[X] = \\sigma^2\\left(\\frac{1}{N} + \\frac{1}{M}\\right)$ and \n",
    "\n",
    "\\begin{align*}\n",
    "T \\sim G\\left(0, \\sigma^2\\left(\\frac{1}{N} + \\frac{1}{M}\\right)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example 1</font> The city of Gainesville claims the mean commute time on SW 24th Ave from I-75 to UF is 23 minutes with a variance of 50. You traveled that route 10 times over the last two weeks and had an average commute time of 27 minutes. Conduct a hypothesis test to determine whether the City of Gainesvilleâ€™s model is reasonable. Reject the null hypothesis if $p < 0.01$.**\n",
    "\n",
    "1. What is the null hypothesis? Define the density under $H_0$.\n",
    "\n",
    "<!-- Null Hypothesis: city's model is correct.\n",
    "\n",
    "\\begin{align*}\n",
    "X_i \\sim \\text{Gaussian}(23, \\sigma_X^2 = 50)\n",
    "\\end{align*} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the sample mean, $\\hat{\\mu}$. Compute the bias and variance of the estimator $\\hat{\\mu}$.\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "\\hat{\\mu} = \\frac{1}{10} \\sum_{i=1}^{10} X_i\\text{, (sample mean estimator)}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "E[\\hat{\\mu}] = 23 = \\mu\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "Var[\\hat{\\mu}] = \\sigma^2_{\\mu_X} = \\frac{\\sigma_X^2}{10} = \\frac{50}{10} = 5\n",
    "\\end{align*} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the probability that observe a result this extreme, i.e., $P(\\hat{\\mu} \\geq 27)$? Compute the one-sided and the two-sided hypothesis test probabilities.\n",
    "\n",
    "<!-- One-sided hypothesis test:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\hat{\\mu} \\geq 27) = P\\left(\\frac{\\hat{\\mu} - 23}{\\sigma_{\\mu_X}} \\geq \\frac{27 - 23}{\\sigma_{\\mu_X}}\\right) = P\\left( Z \\geq \\frac{27-23}{\\sqrt{5}} \\right) = Q\\left(\\frac{27-23}{\\sqrt{5}}\\right) = Q\\left(\\frac{4}{\\sqrt{5}}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Two-sided hypothesis test:\n",
    "\n",
    "\\begin{align*}\n",
    "P(|\\hat{\\mu}| \\geq 27) = 2Q\\left(\\frac{4}{\\sqrt{5}}\\right)\n",
    "\\end{align*} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "<!-- We cannot reject the null hypothesis because p>0.01. The Gainesville's model is correct. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>T-Test</font>: Binary Hypothesis Tests with *Unknown Variance*\n",
    "\n",
    "In many cases, the variance(s) of the underlying distributions are not known and must be estimated from the data.\n",
    "\n",
    "In this case, the underlying distribution is even more spread out from the mean than the Gaussian distribution. More of the probability is in the tails.\n",
    "\n",
    "The first step is to determine how to estimate the variance. Any ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's generate 10 samples from a Gaussian RV with mean 10 and variance 100. \n",
    "    * Compute the sample variance. \n",
    "    * Let's do this for 10,000 simulation steps, during each of which we redraw the 10 random samples and estimate the sample mean and variance. \n",
    "    * Using the average of the sample variance over the 10000 simulations as an estimator of the true variance, what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims=10_000\n",
    "num_samples=10\n",
    "\n",
    "sum_mux=0\n",
    "sum_varx=0\n",
    "sum_varx_biased=0\n",
    "\n",
    "for sim in range(num_sims):\n",
    "    # draw random samples from a G(10,100)\n",
    "    \n",
    "    # compute the sample mean and add it to sum_mux\n",
    "    \n",
    "    \n",
    "    \n",
    "    # compute the biased estimator of the variance and add it to sum_varx\n",
    "    \n",
    "    \n",
    "    \n",
    "    # compute the unbiased estimator of the variance and add it to sum_varx\n",
    "    \n",
    "    \n",
    "    \n",
    "print('The smaple average is ~-=',   ) \n",
    "print('The unbiased variance is ~=',  ) \n",
    "print('The biased variance is ~=',  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use our *unbiased* estimator for the variance, then the distribution of\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\hat{\\mu}-\\mu}{S_{N-1}/\\sqrt{N}}\n",
    "\\end{align*}\n",
    "\n",
    "has a **Studentâ€™s $t$-distribution with $N-1$ degrees of freedom (dof)**.\n",
    "\n",
    "* The density and distribution functions for the **Student's $t$-distribution** are shown on its [Wikipedia page](https://en.wikipedia.org/wiki/Student's_t-distribution).\n",
    "\n",
    "* Unlike the Gaussian distribution, the distribution function for Studentâ€™s t-distribution is in closed form for several values of $\\nu$ (degrees of freedom or dof).\n",
    "\n",
    "* Let's compare the density function of a Normal RV with the Student's $t$ RV with different degrees of freedom.\n",
    "     * Why does it behave this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example 1</font> Analytical Test on Difference of Means (T-Test)**\n",
    "\n",
    "Conduct an **analytical** binary hypothesis test on whether urban and rural populations have different firearms mortality rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the data \"firearms-urban.csv\" where the columns of interest for this investigation are:\n",
    "\n",
    "* **RATE-2014**: The firearms mortality rate by state from 2014. \n",
    "\n",
    "* **Percent Urban**: The percentage of the total population in urban areas, from https://www.icip.iastate.edu/tables/population/urban-pct-states. Although this data is 2010, it should be sufficiently accurate for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the STATE column as the index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "# Highlight the point for Florida\n",
    "\n",
    "plt.scatter(df['Percent Urban'],df['RATE-2014'])\n",
    "plt.scatter(df.loc['FL']['Percent Urban'],df.loc['FL']['RATE-2014'],marker='*',c='r')\n",
    "plt.xlabel('Percent of population in urban areas',size=15)\n",
    "plt.ylabel('Firearms mortality rate',size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's consider:\n",
    "# Urban - states with \"Percent Urban\">=80%\n",
    "# Rural - states with \"Percent Urban\"<80%\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Percent of population in urban areas',size=15)\n",
    "plt.ylabel('Firearms mortality rate',size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's the firearm mortality rate for 2014 where:\n",
    "# Urban - states with \"Percent Urban\">=80%\n",
    "# Rural - states with \"Percent Urban\"<80%\n",
    "\n",
    "# Extract data\n",
    "\n",
    "\n",
    "# Estimate Density using KDE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "x = np.linspace(0,22,1000)\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Firearms Mortality Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's construct a **two-sided binary hypothesis test** using analytical techniques and determine the probability of such a large difference in means under the null hypothesis.\n",
    "\n",
    "The data comes from a single distribution, which implies same means and same variances.\n",
    "\n",
    "So, we will use the **T-random variable** to model this. We know: mean of $T$ is 0, and we need to calculate the variance from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample unbiased estimator for the variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to calculate the variance of the difference of sample mean estimators\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last parameter of T: degrees of freedom\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the t-distribution and perform the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# We set the variance of the T random variable here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-sided test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-sided test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other 1-sided\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "<!-- The p-value is much smaller than $\\alpha=0.01$, therefore we reject the null hypothesis that the means of the populations for urban firearms mortality rate and rural firearms mortality rate are NOT the same. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example 2</font>** Use the Student's $T$ random variable to determine a 95% confidence interval for the mean difference under the null hypothesis. Is the resulting confidence interval compatible with the observed difference of means?\n",
    "\n",
    "*Hint:* The inverse CDF function in ```scipy.stats``` is called the Percent point function (PPF) and is given by the ```ppf``` method of random variable objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The 95% confidence interval is the interval $[a,b]$ for which $P(T \\le a) =0.025$ and $P(T\\ge b) = 0.025$\n",
    "\n",
    "Thus\n",
    "$P(T \\le a) = F_T(a) =0.025$, which is satisfied if $a = F_{T}^{-1}(0.025)$\n",
    "\n",
    "Similarly,\n",
    "$P(T \\leq b) = 0.975$ and $P(T \\ge b) = 1 - F_T(b) +P(T=b) =1- F_T(b) =0.025$, which is satisfied if \n",
    "$b= F_{T}^{-1}(0.975)$\n",
    "\n",
    "Thus, the 95% confidence interval is: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "<!-- Since the 95% confidence interval does not contain the observed mean difference, we say it is not compatible with the assumption that this data comes from the same distribution (in particular, it is incompatible with even the means of the distributions being equal). Therefore this is a statistically significant event, therefore we reject the null hypothesis. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors and Performance Tradeoffs in Hypothesis Testing\n",
    "\n",
    "* In binary hypothesis testing, there are two types of errors:\n",
    "\n",
    "    1. **False Alarm** (Type I Error, also called *False Positive*)\n",
    "        * occurs if we accept a hypothesis when it is not true\n",
    "        * we will use the notation\n",
    "        \\begin{align*}\n",
    "        P_{fa} = P(\\text{false alarm})\n",
    "        \\end{align*}\n",
    "    \n",
    "    2. **Miss** (Type II Error, also called *False Negative*)\n",
    "        * occurs if we reject a hypothesis when it is actually true\n",
    "        * we will use the notation\n",
    "        \\begin{align*}\n",
    "        P_m = P(\\text{miss})\n",
    "        \\end{align*}\n",
    "\n",
    "* When performing a hypothesis test, there is always a tradeoff between these two types of errors\n",
    "\n",
    "* The tradeoff is controlled by choosing the significance level, $\\alpha$, to which the p-value is compared with\n",
    "    * the value $\\alpha$ is the probability that we will reject the null hypothesis, $H_0$ when it is in fact true\n",
    "    * equivalently, it is the probability of accepting the alternative hypothesis, $H_1$, when $H_1$ is false\n",
    "    \n",
    "* Even though the binary hypothesis test is usually conducted under the assumptions of $H_0$, we are usually conducting it to determine whether $H_1$ is the cause of the observed difference\n",
    "    * thus, we will consider the implications with respect to $H_1$ when labeling errors\n",
    "    * so for the case that we accept $H_1$ when it is false, we call that a **false alarm/Type I error**\n",
    "    * then $P_{fa} = \\alpha$\n",
    "    \n",
    "* Note that if we decrease $\\alpha$, then we decrease $P_{fa}$, but we also decide that the null hypothesis could be true when it is in fact false\n",
    "    * i.e., we increase the **Probability of Miss/Type II error**, $P_m$\n",
    "\n",
    "* The converse is also true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Decisions from Continuous Data\n",
    "\n",
    "* We have many situations where we have a continuous measurement that depends on some underlying binary phenomena.\n",
    "\n",
    "* For example, we may wish to determine the presence of a disease based on the measurement of some chemical\n",
    "     * Then the distribution of the data depends on whether the disease is present or not.\n",
    "     \n",
    "* More generally, we assume the data comes from one of two continuous densities, $f_0(x|H_0)$ or $f_1(x|H_1)$, and we wish to make a decision between $H_0$ and $H_1$ based on an observed value $x$\n",
    "\n",
    "* We will choose $H_i$ if $x\\in R_i$, where $R_0$, $R_1$ are partitions of the real line\n",
    "\n",
    "* The probability of false alarm and probability of miss then depend on the decision regions $R_0$ and $R_1$\n",
    "\n",
    "* In many cases, the decision regions are determined by a single threhold $\\gamma$, like $R_0 = x < \\gamma$ and $R_1 = x > \\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example 1</font>**\n",
    "The PSA (Prostate-Specific Antigen) values for men in their 60s without cancer are approximately Gaussian(2,$\\sigma^2=1$). The PSA values for men in their 60s with cancer are approximately Gaussian(4,$\\sigma^2=2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawMAP(p0): \n",
    "    # Setup RVs\n",
    "    \n",
    "    \n",
    "    \n",
    "    x=np.linspace(-1,8,1001)\n",
    "    p1=1-p0 # prior probability (p0 is given)\n",
    "        \n",
    "    # plot the weighted densities:\n",
    "    # these are proportional to the APPs\n",
    "    plt.plot(x,p0*G0.pdf(x),label='$f_X(x|H_0)P(H_0)$')\n",
    "    plt.plot(x,p1*G1.pdf(x),label='$f_X(x|H_1)P(H_1)$')\n",
    "    \n",
    "    # Determine the regions where the APP for 0 is \n",
    "    # bigger and the APP for 1 is bigger\n",
    "    R0=x[np.where(p0*G0.pdf(x)>= p1*G1.pdf(x))]\n",
    "    R1=x[np.where(p0*G0.pdf(x)< p1*G1.pdf(x))]\n",
    "\n",
    "    # Fill under the regions found above\n",
    "    plt.fill_between(R0,p0*G0.pdf(R0),alpha=0.3,label='Decide $H_0$')\n",
    "    plt.fill_between(R1,p1*G1.pdf(R1),alpha=0.3,label='Decide $H_1$')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Print the MAP threshold\n",
    "    print('Reject the Null Hypothesis H0 if PSA is >',round(R1[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For $P_{fa} = 10$%, find $P_m$\n",
    "    \n",
    "<!-- \\begin{align*}\n",
    "P_{fa} &= 0.1 \\\\\n",
    "Q\\left(\\frac{\\gamma-2}{\\sqrt{\\sigma^2}}\\right) &= 0.1 \\\\\n",
    "Q\\left(\\frac{\\gamma-2}{\\sqrt{1}}\\right) &= 0.1 \\\\\n",
    "\\gamma - 2 &= Q^{-1}(0.1) \\\\\n",
    "\\gamma - 2 &= 1.28 \\\\\n",
    "\\gamma &\\approx 3.28\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "P_m = Q\\left(\\frac{4 - 3.28}{\\sqrt{2}}\\right) \\approx 0.305\n",
    "\\end{align*} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For $P_m = 10$%, find $P_{fa}$\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "P_m &= 0.1 \\\\\n",
    "Q\\left(\\frac{4 - \\gamma}{\\sqrt{2}}\\right) &= 0.1 \\\\\n",
    "\\frac{4-\\gamma}{\\sqrt{2}} &\\approx 1.28\\\\\n",
    "\\gamma &\\approx 2.19\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "P_{fa} = Q\\left(\\frac{2 - 2.19}{1}\\right) \\approx 0.425\n",
    "\\end{align*} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Tradeoffs in Hypothesis Testing: ROC Curves\n",
    "\n",
    "* We can visualize the relation between these types of errors using a ROC curve\n",
    "    * ROC stands for *receiver operating characteristic*\n",
    "    * ROC curves were developed for RADAR systems but are widely used in fields of statistical tests, such as biomedicine\n",
    "\n",
    "* In ROC curves, we do not plot $P_{fa}$ vs $P_m$\n",
    "\n",
    "* Instead:\n",
    "    * the x-axis is **FPR (false positive rate)**\n",
    "    \\begin{align*}\n",
    "    \\text{FPR}=P_{fa}\n",
    "    \\end{align*}\n",
    "    \n",
    "    * the y-axis is **TPR (true positive rate)**\n",
    "    \\begin{align*}\n",
    "    \\text{TPR}=1-P_m\n",
    "    \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under the Curve (AUC)\n",
    "\n",
    "*Area Under Curve (AUC)* is a common measure of how good a test is. It is simply the area under the ROC curve. Random guessing can achieve the diagonal line, so the minimum AUC is 1/2. The maximum AUC is 1, which is achieved by a test that is always right; the ROC curve is along the left and top axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example 2</font>** Plot the performance if the variance of each PSA test is reduced by a factor of 4. What is the AUC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Goodness-of-Fit\n",
    "\n",
    "The goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Whether Data Comes from a Distribution: Discrete Distributions\n",
    "\n",
    "* Given a set of random data and a proposed model, how could we determine if the data could have reasonably come from that model?\n",
    "\n",
    "* For example, given values from a six-sided die, how could you tell if the die is fair? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>Example 3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 60 values from a fair 6-sided die:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Even after 60 rolls, the numbers still vary significantly\n",
    "\n",
    "* We could compare them to the expected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>Example 4</font>\n",
    "\n",
    "The file \"baseball.pickle\" contains the birth months of major league baseball players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [```pickle```](https://docs.python.org/3/library/pickle.html) is a Python object serialization library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations and Comments:\n",
    "\n",
    "* Note that more MLB players are born in August than any other month\n",
    "\n",
    "* Some people claim that this is because in little league baseball through 2006, the cutoff for determing a player's age eligibility was July 31st\n",
    "\n",
    "* That is, a player who was turning 9 that year would not be eligible to play in the 8 & Under league if their birthday was before August 1st. \n",
    "\n",
    "* Thus, players with July birthdays were the youngest (and, on average, smallest) in their leagues, and player with August birthdays were the oldest (and, on average, largest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by comparing the data values in the cells to the expected values for those cells \n",
    "\n",
    "* **We assume a uniform distribution of birthdays over the year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get the expected number of birthdays in a month as the probability a player is born in a month (which is just the number of days in the month divided by 365) times the total number of players in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_month=np.array([31,28,31,30,31,30,31,31,30,31,30,31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the data to the expected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* August through November seem to be overrepresented, but how can we test it, with 12 different values?\n",
    "\n",
    "* Let's start by computing the errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we need to turn the errors into a single test statistic\n",
    "\n",
    "* Note that the errors are both positive and negative\n",
    "\n",
    "* We solve this in the same way we have before, let's start by looking at the total squared error (also called the total deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we can carry out our statistical test in the usual way\n",
    "\n",
    "* We draw examples from the distribution under $H_0$ and then see how often we get such a large total deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims=1000\n",
    "count=0\n",
    "for sim in range(num_sims):\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "print('Prob of seeing a result this extreme is', count/num_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "<!-- The result is statistically significant. MLB players' birthdays are not uniformly distributed throughout the year. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that some months have more days than others. Those months will naturally have more variation than months with more days because the expected counts will be smaller\n",
    "\n",
    "* To compensate for this effect, it is instead common to normalize the cell deviations by dividing by the expected value of that cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalizing, we can again calculate a statistic that is a sum of the normalized deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reasons we will discuss later, this is called the **chi-squared statistic** with $N-1$ degrees of freedom (dof), i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N \\frac{(O_i - E_i)^2}{E_i} \\sim \\chi^2(\\text{dof}= N-1)\n",
    "\\end{align*}\n",
    "\n",
    "where $O_i$ is the observed value, $E_i$ is the expected value and $N$ is the total number of random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,20,1000)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,50,1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('p-value = ', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can carry out a similar simulation test as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims=1000\n",
    "count=0\n",
    "for sim in range(num_sims):\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "print('Prob of seeing a result this extreme is',count/num_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "<!-- The result is statistically significant. MLB players' birthdays are not uniformly distributed throughout the year. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>Example 2</font>\n",
    "\n",
    "Instead, let's try another baseball example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(From *Mathematical Statistics with Resampling and R* By Laura M. Chihara, Tim C. Hesterberg)\n",
    "\n",
    "The file \"homeruns.pickle\" contains the homerun data for the Philadelphia Phillies in 2009.\n",
    "\n",
    "Each entry is the number of games with the corresponding index number of homeruns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What distribution might this come from??\n",
    "<!--     * Poisson? -->\n",
    "\n",
    "* What do we need to specify that distribution?\n",
    "<!--     * Need the average number of HRs/game (i.e., the mean of the distribution) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a good fit. BUT we should test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think that this probably comes from this Poisson distribution. If so, the simulation should produce a p-value >> 0.055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims=1000\n",
    "count=0\n",
    "for sim in range(num_sims):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "print('Prob of seeing a result this extreme is',count/num_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "<!-- It is likely that this data matches a Poisson distribution. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Whether Data Comes from a Distribution: Continuous Distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>Example 3</font> \n",
    "\n",
    "Consider the data in \"lightbulb.pickle\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we determine what distribution this data comes from?\n",
    "\n",
    "* Let's look at what sorts of values we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values seem to be coming from throughout the positive real line -- this data is from a continous distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to try to determine which continuous distribution is a good fit for the data.\n",
    "\n",
    "1. Start by plotting a histogram of the data. Adjust the number of bins to provide an appropriate amount of resolution to help infer what distribution this might be from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the data is not Gaussian/Normal or Uniform. \n",
    "\n",
    "* Of the distributions we have considered, this seems to match the exponential random variable. \n",
    "\n",
    "Assuming that this is from an exponential distribution, we can compare the histogram of the data with that from the theoretical model. The exponential distribution is characterized by a single parameter, either $\\lambda$ or $\\mu=1/\\lambda$, which is the mean.\n",
    "\n",
    "2. Let's estimate the mean of the reference distribution. We know that the sample mean estimator is an unbiased estimate of the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now create an exponential random variable object and draw data from this reference distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match is not perfect, but they are similar. \n",
    "\n",
    "* Since this data comes from a continuous distribution, kernel density estimation (KDE) would be even better than a histogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,3500,10000)\n",
    "\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile-Quantile (Q-Q) Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate other ways that we can visually compare these data. We will first generate a **quantile-quantile (Q-Q) plot** for the data.\n",
    "\n",
    "* The $k$th **quantile** from a data set of length $n$ is the data point that is $k/n$th of the way through the ordered set.  \n",
    "\n",
    "* In a Q-Q plot, we plot the data at a particular quantile in one data set vs the data at a particular quantile in another data set. \n",
    "\n",
    "Read the wikipedia page on Q-Q plots: https://en.wikipedia.org/wiki/Qâ€“Q_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider the easiest case, which is when the data sets are of the same size. In that case, we can just plot the sorted values with respect to each other:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is from the same distribution, the plot should be approximately linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are comparing data to a known distribution, we can get the exact quantiles from the distribution of the random variable, rather than using samples from the random variable. \n",
    "\n",
    "Sometimes this type of plot is called a **probability plot**, and I will use that terminology in this class. More generally, the term probability plot is sometimes used to refer to a broader class of plots including the Q-Q plot. **We can get the quantiles from a distribution using the inverse CDF (in ```scipy.stats```, this is called the percent point function (ppf):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the function is even more linear. It has a little less variation because we have eliminated one of the sources of randomness in the Q-Q plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we leverage the ```scipy.stats``` ```probplot``` method to generate the same plot directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the docstring for the ```stats.probplot``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can only use ```stats.probplot``` for distributions that ```scipy.stats``` knows (but that is A LOT).\n",
    "\n",
    "After you have read the docstring and understand the outputs, let's store those and look at them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not yet ready to talk in detail about linear regression, but basically it is finding the best line to fit a set of data (when the error is mean-squared error).\n",
    "\n",
    "The regression parameters are (from the docstring): (slope, intercept, r)\n",
    "\n",
    "You all should be familiar with the slope and intercept of a line. The parameter ```r``` (usually written $r$ in text) measures how close the data fits the line. We will work with $r^2$ instead. The closer $r^2$ is to 1, then the better the line fits the quantiles (and the better our reference distribution fits the data). We will consider the reference distribution to be a good match for the data if $r^2 \\ge 0.9$.\n",
    "\n",
    "Let's check how well the exponential distribution fits our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "<!-- Since $r^2 \\approx 0.9979 > 0.9$, the exponential distribution is an excellent fit to this data set. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is a Contingency Table?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Contingency Table</strong>\n",
    "\n",
    "A **contingency table**, sometimes called *cross-tabulation* or *two-way table*, is a type of table in a matrix format that displays (multivariate) categorical data in terms of frequency counts.\n",
    "\n",
    "More precisely, an $r\\times c$ contingency table shows the observed frequency of two variables, the observed frequencies of which are arranged into $r$ rows and $c$ columns. The intersection of a row and a column of a contingency table is called a cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Contingency tables are great to summarize (large) data sets\n",
    "\n",
    "* Contingency tables are used for organizing categorical variables and testing hypothesis with the chi-squared test for independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, the contingency table below has two rows and five columns (not counting header rows/columns) and shows the results of a random sample of 2200 adults classified by two variables, namely gender and favorite way to eat ice cream.\n",
    "\n",
    "|  |   cup   |  cone   | sundae  | sandwich |  other  |\n",
    "|--------|---------|---------|---------|----------|---------|\n",
    "|  male  |   592   |   300   |   204   |    24    |    80   |\n",
    "| female |   410   |   335   |   180   |    20    |    55   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One benefit of having data presented in a contingency table is that it allows one to more easily perform basic probability calculations, a feat made easier still by augmenting a summary row and column to the table.\n",
    "\n",
    "|  |   cup   |  cone   | sundae  | sandwich |  other  |  total  |\n",
    "|--------|---------|---------|---------|----------|---------|---------|\n",
    "|  male  |   592   |   300   |   204   |    24    |    80   |   1200  |\n",
    "| female |   410   |   335   |   180   |    20    |    55   |   1000  |\n",
    "|  total |   1002  |   635   |   384   |    44    |   135   |   2200  |\n",
    "\n",
    "The above table is an extended version of the first table obtained by adding a summary row and column. These summaries allow easier computation of several different probability-related quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Marginal Total\n",
    "\n",
    "The numbers in every cell are called **marginal totals**. The grand total (the total number of individuals represented in the contingency table) is the number in the bottom right corner.\n",
    "\n",
    "The table allows users to see at a glance that the proportion of men who like to eat their ice cream in a cone is about the same as the proportion of women who like to eat their ice cream in a cone although the proportions are not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Probability\n",
    "\n",
    "If the proportions of individuals in the different columns vary significantly between rows (or vice versa), it is said that there is a *contingency* between the two variables. In other words, the two variables are **not independent**. If there is no contingency, it is said that the two variables are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expected Frequency\n",
    "\n",
    "One useful value to know is the **expected frequency** $E_{r,c}$ of the cell at the intersection of column c and row r, the formula for which is given by\n",
    "\n",
    "$$E_{c,r} = \\frac{\\text{(sum of row }r\\text{)}\\times\\text{(sum of column }c\\text{)}}{\\text{sample size}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <font color=blue>Example 2</font> \n",
    "\n",
    "From the contingency table below, compute:\n",
    "\n",
    "|  |   cup   |  cone   | sundae  | sandwich |  other  |  total  |\n",
    "|--------|---------|---------|---------|----------|---------|---------|\n",
    "|  male  |   592   |   300   |   204   |    24    |    80   |   1200  |\n",
    "| female |   410   |   335   |   180   |    20    |    55   |   1000  |\n",
    "|  total |   1002  |   635   |   384   |    44    |   135   |   2200  |\n",
    "\n",
    "1. Probability that a random participant prefers their ice cream in a cup? \n",
    "\n",
    "<!-- $$\\frac{1002}{2200}$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Probability that a random participant prefers their ice cream in a sandwich? \n",
    "\n",
    "<!-- $$\\frac{44}{2200}$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Probability that a random participant is female? \n",
    "\n",
    "<!-- $$\\frac{1000}{2200}$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Probability that a person prefers ice cream sandwiches given that the person is male? \n",
    "\n",
    "<!-- $$P(\\text{sandwich}|\\text{male}) = \\frac{P(\\text{sandwich} \\cap \\text{male})}{P(\\text{male})} = \\frac{24}{1200}$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. Probability that a person is male given that ice cream sandwiches are preferred? \n",
    "\n",
    "<!-- $$P(\\text{male}|\\text{sandwich}) = \\frac{24}{44}$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "6. Expected value of men who prefer to eat ice cream from a cup? \n",
    "\n",
    "<!-- $$\\frac{1200 \\times 1002}{2200} \\approx 546.54$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "7. Expected value of women who prefer to eat ice cream from a sundae? \n",
    "\n",
    "<!-- $$\\frac{1000 \\times 384}{2200} \\approx 174.54$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What are the variables of study? \n",
    "\n",
    "<!-- Gender and preferred way to eat ice cream. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How many degrees of freedom does this contingency table have? \n",
    "\n",
    "<!-- $$(2-1) \\times (5-1) = 4$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Chi-Squared Test & Independence\n",
    "\n",
    "One of the major benefits of computing expected frequencies is the ability to test whether the two variables are actually *independent*. This is done by computing, for each cell (c,r), the expected frequency $E_{c,r}$, comparing it to the observed frequency $O_{c,r}$, and then performing the **chi-squared test**.\n",
    "\n",
    "$$\\chi^2 = \\sum_{\\text{all cells}} \\frac{(O_{c,r}-E_{c,r})^2}{E_{c,r}} = \\sum_c \\sum_r \\frac{(O_{c,r}-E_{c,r})^2}{E_{c,r}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This statistic is called a $\\chi^2$ as it follows a [$\\chi^2$ distribution with $k$ degrees of freedom](https://en.wikipedia.org/wiki/Chi-squared_distribution).\n",
    "\n",
    "* The degrees of freedom can be computed as $(\\text{# rows}-1)\\times (\\text{# columns}-1)$\n",
    "\n",
    "* In the example above, there are 4 degrees of freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,20,1000)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for i in range(2,11):\n",
    "    C = stats.chi2(i)\n",
    "    plt.plot(x,C.pdf(x),label='k = '+str(i))\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Chi-Squared ($\\chi^2$) RV and Gaussian RV\n",
    "\n",
    "If $Y_1, \\dots, Y_k$ independent identically distributed (i.i.d.), standard normal random variables, that is, $Y_i \\sim G(0,1)$, then\n",
    "\n",
    "$$Z = \\sum_{i=1}^k Y_i^2$$\n",
    "\n",
    "$Z$ is distributed as $\\chi^2$ with $k$ degrees of freedom, $Z \\sim \\chi^2(k)$.\n",
    "\n",
    "In other words, the chi-square distribution with $k$ degrees of freedom is the distribution of a sum of the squares of $k$ independent Normal random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N = 10_000\n",
    "x = np.linspace(0,20,N)\n",
    "\n",
    "dof = 5\n",
    "C = stats.chi2(dof)\n",
    "G = stats.norm()\n",
    "\n",
    "sum_kG = 0\n",
    "for i in range(dof):\n",
    "    #\n",
    "    #\n",
    "    \n",
    "plt.plot(x,C.pdf(x),label='$\\chi^2$(dof='+str(dof)+')')\n",
    "plt.plot(x, stats.gaussian_kde(sum_kG)(x), label='KDE estimate for $\\chi^2=(dof=$'+str(dof)+')')\n",
    "plt.hist(sum_kG,density='True', label='Sum of '+str(dof)+' Normal dist.')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <font color=blue>Example 3</font> \n",
    "\n",
    "Carry the independence test for the two variables in the following contingency tables:\n",
    "\n",
    "|  |   cup   |  cone   | sundae  | sandwich |  other  |  total  |\n",
    "|--------|---------|---------|---------|----------|---------|---------|\n",
    "|  male  |   592   |   300   |   204   |    24    |    80   |   1200  |\n",
    "| female |   410   |   335   |   180   |    20    |    55   |   1000  |\n",
    "|  total |   1002  |   635   |   384   |    44    |   135   |   2200  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "* The columns and rows sum to 0\n",
    "* Cannot use the sum of the error (defined as the different between observed and expected) as it always sums to 0. Instead let's consider the squared of the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Some cells have larger expected values and more observations than others\n",
    "\n",
    "* A difference of 10 in a cell of expected value of 5 is more significant than a difference of 10 in a cell of expected value of 100\n",
    "\n",
    "* We can take into account the expected cell size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this statistic, how can we determine if this result is statistically significant with $\\alpha=0.01$? We need to find the $p$-value under the null Hypothesis $$H_0:\\text{variables gender and favorite way to eat ice cream are independent}$$\n",
    "\n",
    "There are two approaches: \n",
    "1. Resampling (permutation)\n",
    "2. Analytical Solution (Chi-Squared test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical Solution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling solution\n",
    "# 0 - cup, 1 - cone, 2 - sundae, 3 - sandwich, 4 - other\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims=10_000\n",
    "count=0\n",
    "\n",
    "for i in range(num_sims):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Probability of observing a table this extreme under H0 is ~\",count/num_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Conclusion:** \n",
    "\n",
    "<!-- We reject the null hypothesis. The two variables are **not** independent. -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
